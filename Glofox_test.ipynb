{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "742fbc4a",
   "metadata": {},
   "source": [
    "# Importing Libraries and Creating Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0aff3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import shutil\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "import shutil\n",
    "from os import path\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d220d",
   "metadata": {},
   "source": [
    "# Cleaning the Data and Creating the Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5db0f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeChar(text):\n",
    "    \n",
    "    remove = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
    "    lower_result = text.lower()\n",
    "\n",
    "    for char in remove:\n",
    "        lower_result = lower_result.replace(char, '')\n",
    "\n",
    "    return lower_result\n",
    "\n",
    "def splitRDD(rdd):\n",
    "    return rdd.map(removeChar) \\\n",
    "            .flatMap(lambda text: text.split(' ')) \\\n",
    "            .filter(lambda text: text != '')\n",
    "\n",
    "path = 'dataset/*'\n",
    "\n",
    "rdd = spark.sparkContext.textFile(path)\n",
    "final_rdd = splitRDD(rdd)\n",
    "dictionary = final_rdd.distinct() \\\n",
    "            .zipWithIndex()\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(dictionary, [\"word\", \"wordId\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c703602",
   "metadata": {},
   "source": [
    "# Applying Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f2e42d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(wordId=0, docId=[25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]), Row(wordId=1, docId=[25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]), Row(wordId=2, docId=[25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]), Row(wordId=3, docId=[25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]), Row(wordId=4, docId=[25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25])]\n"
     ]
    }
   ],
   "source": [
    "path = 'dataset/'\n",
    "files = []\n",
    "\n",
    "for name in os.listdir(path):\n",
    "    file = os.path.join(path, name)\n",
    "\n",
    "    if os.path.isfile(file):\n",
    "        files.append(int(name))\n",
    "\n",
    "\n",
    "\n",
    "list_files = []\n",
    "\n",
    "for file in files:\n",
    "    rdd = spark.sparkContext.textFile(path + str(file))\n",
    "    rdd = splitRDD(rdd).distinct()\n",
    "    rdd = rdd.map(lambda word: (word, file))\n",
    "    list_files.append(rdd)\n",
    "\n",
    "\n",
    "pre_agg_df = files_df.join(\n",
    "    dictionary_df, dictionary_df.word == files_df.word, how='left')\n",
    "\n",
    "\n",
    "\n",
    "files_and_rdd = spark.sparkContext.union(list_of_rdd)\n",
    "files_and_df = spark.createDataFrame(files_and_rdd, ['word', 'docId'])\n",
    "\n",
    "final_df = joinFilesDict(files_and_df, df)\n",
    "\n",
    "inverted_index = final_df \\\n",
    "    .groupBy(F.col('wordId')) \\\n",
    "    .agg(F.sort_array(F.collect_list(F.col('docId'))).alias('docId')) \\\n",
    "    .orderBy(F.col('wordId'))\n",
    "\n",
    "\n",
    "print(inverted_index_df.head(5))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
